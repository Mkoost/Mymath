\documentclass[12pt, a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{geometry}
\usepackage{mathtools}
\usepackage{verbatim}
\usepackage{indentfirst}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{float}

\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\usepackage[oglav,spisok,boldsect,eqwhole,figwhole,hyperref,hyperprint,remarks,greekit]{./style/fn2kursstyle}

\graphicspath{{./style/}{./figures/}}

\frenchspacing

\title{Итерационные методы решения систем линейных алгебраических уравнений}
\lab{3}
\author{М.\,А.~Каган}
\creator{И.\,А.~Яковлев}
\supervisor{}
\group{ФН2-51Б}
\date{2024}

\begin{document}
	\maketitle
	\tableofcontents
	
	\newpage
	

	
	\section-{Контрольные вопросы}
	
	\begin{enumerate}
	\item \textbf{Почему нельзя находить собственные числа матрицы $A$, прямо решая уравнение $\text{det}\left( A - \lambda E\right) = 0$, а собственные векторы -- <<по определению>>, решая систему $(A - \lambda_j E) e_j = 0$?}
	\vspace*{0.2cm}
	
	\textit{\textbf{Ответ:}}
	
	Для матрицы с рангом $n$ раскрытие определителя $\text{det}\left( A - \lambda E\right) = 0$ приведет к уравнению относительно $\lambda \, \, n-$ой степени, для больших по величине $n$ которого решения в явном виде не существует. Кроме того, для построения такого многочлена может требоваться значительное число операций, в результате чего сначала возникнут погрешности в составленном уравнении, а впоследствии и в его решении.
	
	Ввиду того, что собственные значения $\lambda_j$ матрицы $A$ ищутся лишь приближенно, при попытке решить систему вида $(A - \lambda_j E) e_j = 0$ матрица $(A - \lambda_j E)$ не будет вырождена, и алгоритм поиска решения сможет найти лишь тривиальное решение.
	
	\item \textbf{Докажите, что ортогональное преобразование подобия сохраняет симметрию матрицы.}
	\vspace*{0.2cm}
	
	\textit{\textbf{Ответ:}}
	
	Пусть $A$ --- симметричная матрица, т.е. $A = A^\text{T}$; $Q$ --- ортогональная матрица. По свойству ортогональных матриц:  $Q^{-1} = Q^\text{T}$. Рассмотрим ортогональное подобное преобразование :
	\begin{gather}
		B = Q A Q^{-1} = Q A Q^\text{T} \notag \\
		B^\text{T} = (Q A Q^\text{T})^\text{T} = (A Q^\text{T})^\text{T} Q^\text{T} = Q A^\text{T} Q^\text{T} = Q A Q^\text{T} = B \notag
	\end{gather}
	
	\item \textbf{Как преобразование подобия меняет собственные векторы матрицы?}
	\vspace*{0.2cm}
	\textit{\textbf{Ответ:}}
	
	Матрица $B$ подобна матрице $A$: $B = P^{-1} A P$. Подобие сохраняет собственные числа исходной матрицы, то есть $A$ и $B$ имеют общий набор СЗ $ \{ \lambda_j \}$. Рассмотрим преобразование собственных векторов; для матрицы $A$ собственные векторы $e_j$ предлагается искать в виде:
	$$
	A e_j = \lambda_j e_j,
	$$
	тогда для матрицы B равенство примет вид
	$$
	P^{-1} A P e^{B}_j = \lambda_j e^B_j, \implies A \left( P e^B_j \right) = \lambda_j (P e^B_j),
	$$
	откуда преобразование собственных векторов можно задать в виде $e^B_j = P^{-1} e_j$.
	
	
	
	\item \textbf{Почему на практике матрицу $A$ подобными преобразованиями вращения приводят только к форме Хессенберга, но не к треугольному виду?}
	
	\vspace*{0.2cm}
	
	\textit{\textbf{Ответ:}}
	\begin{enumerate}
		\item Могут <<потеряться>> комплексные собственные числа, поскольку матрицы поворота состоят из действительных чисел;
		\item Из предыдущего пункта следует, что нет явного способа привести матрицу к треугольному виду с использованием арифметики действительных чисел; 
		\item QR-алгоритм сохраняет форму Хессенберга на каждой итерации;
	\end{enumerate}

	
	\item \textbf{Оцените количество арифметических операций, необходимое для приведения произвольной квадратной матрицы A
		к форме Хессенберга.}
	\vspace*{0.2cm}
	
	\textit{\textbf{Ответ:}}
	
	Будем обозначать $k$ номером строки, $l$ - номером столбца. Для вычисления констант $\alpha$ и $\beta$ необходимо произвести минимум две операции умножения и две операции деления. Для умножения матрицы $A$ на матрицу $T_{k \,l}$ достаточно заменить строки $k$ и $l$ на их линейные комбинации, для чего требуется умножить каждый элемент $k$-й и $l$-й строки сначала на $\alpha$ и $\beta$, затем на $-\beta$ и $\alpha$ соответственно: в сумме получается $4n$ операции. Для приведения матрицы $A$ к форме Хессенберга необходимо обнулить $\frac{1}{2}(n-1)(n-2)$ элементов, а для обнуления одного элемента необходимо произвести $4n + 4$ операций, следовательно в сумме получается $2 n^3 - 4n^2 - 2n + 24 \sim 2 n^3$.
	
	\item \textbf{Сойдется ли алгоритм обратных итераций, если в качестве начального приближения взять собственный вектор, соответствующий другому собственному значению? Что будет в этой ситуации в методе обратной итерации, использующем отношение Рэлея?}
	
	\vspace*{0.2cm}
	
	\textit{\textbf{Ответ:}}
	\begin{enumerate}
		\item Рассмотрим процесс метода обратных итераций:
		\[
		(A - \lambda^{*}_i E)y_{j+1} = x_j,
		\] 
		 где $\lambda^{*}_i$ приближение к собственному значению $\lambda_i$ и $x_{j} = \sfrac{y_j}{\norm{y_{j}}}$. Пусть $e_k$ собственный вектор не соответствующий собственному значению $\lambda_i$. Тогда:
		 \begin{gather}
		 (A - \lambda^{*}_i E) e_k = \lambda_k e_k - \lambda^*_i e_k = (\lambda_k - \lambda^*_i) e_k, \notag \\
		 \frac{1}{\lambda_k - \lambda^*_i}(A - \lambda^{*}_i E) e_k = (A - \lambda^{*}_i E)\frac{e_k}{\lambda_k - \lambda^*_i} =  e_k.
		 \end{gather}
		 Таким образом, если положить $x_0 = e_k$, тогда $x_1 = e_k$. Следовательно, итерационный процесс не сходится к собственному вектору, соответствующему значению  $\lambda_k$, но сходится за одну итерацию к собственному значению соответствующему вектору $e_k$.
		 
		 \item Рассмотрим процесс метода обратных итераций с соотношением Рэлея:
		 \[
		 (A - \lambda_k E)y_{j+1} = x_j,
		 \] 
		 где $\lambda_k = (A x_k, x_k)$ соотношение Рэлея и $x_{j} = \sfrac{y_j}{\norm{y_{j}}}$. Итерационный процесс должен сходиться к  собственному значению $\lambda$. Тогда, если положить $x_0 = e_k$, тогда матрица $(A - \lambda_k E)$ вырожденная, следовательно нет единственного решения, то есть итерационный процесс нельзя продолжить.
	\end{enumerate}
	
	\item \textbf{Сформулируйте и обоснуйте критерий останова для $QR-$алгоритма отыскания собственных значений матрицы.}
	\vspace*{0.2cm}
	
	\textit{\textbf{Ответ:}}
	
	При выполнении $QR$-алгоритма получается последовательность матриц $A^{(k)}$, сходящихся к верхнетреугольной матрице $R$, на диагонали которой будут стоять искомые собственные значения. При этом элементы $a^{(k)}_{ij}$ матриц $A^{(k)}$, стоящие ниже главной диагонали, сходятся к нулю со скоростью некоторой зависящей от собственных значений геометрической прогрессии. После того, как элементы последней строки под диагональю станут близки к нулю, элемент $a^{(k)}_{nn}$ можно принять за приближение собственного значения $\lambda_n$, приступив для аналогичной задачи матрицы размерности $(n-1) \times (n-1)$.
	
	То есть, критерий останова для $QR$-алгоритма можно сформулировать следующим образом: сумма модулей элементов последней строки матрицы $(l-1) \times (l-1), \, l <= n$, лежащих под главной диагональю, должна быть меньше некоторого фиксированного числа, определяющего точность приближения: \[ \sum_{j=1}^{l - 1} \abs{a^{(k)}_{l j}} < \varepsilon \]
	
	\item \textbf{ Предложите возможные варианты условий перехода к алгоритму со сдвигами. Предложите алгоритм выбора величины сдвига.}
	\vspace*{0.2cm}
	
	\textit{\textbf{Ответ:}}
	
	Сходимость QR алгоритма:
	\[
	\abs{a_{ij}^{(k)}} \le \abs{\dfrac{\lambda_i}{\lambda_j}}\cdot \abs{a_{ij}^{(k-1)}}, \;\;\; i > j, \;\;\; k = 1, 2, \ldots
	\]
	\begin{enumerate}
		\item
		Первый вариант: есть хорошее приближение собственного значения, тогда можно воспользоваться сдвигами, чтобы ускорить алгоритм; второй вариант: если матрица плохо обусловлена, то даже плохое приближение может ускорить процесс. В общем случае также следует применять сдвиги либо изначально, либо начиная с некоторой итерации, поскольку в процессе решения задачи возникают значения близкие к собственным, которые можно использовать как значение сдвига.  
		
		\item Можно воспользоваться теоремой Гершгорина, которая дает следующую оценку собственных значений:
		\[
		\abs{\lambda - a_{ii}} \le \sum\limits_{i \neq j}\abs{a_{ij}}.
		\] 
		Таким образом, при малой сумме элементов матрицы стоящих вне главной диагонали, диагональные элементы будут хорошим приближением к собственным элементам.   
	\end{enumerate}
	
	
	\item \textbf{Для чего нужно на каждой итерации нормировать приближение к собственному вектору?}
	\vspace*{0.2cm}
	
	\textit{\textbf{Ответ:}}
	Пусть $e_k$ -- собственный вектор для некоторого собственного значения $\lambda_k$ матрицы $A$ :
	$$
	Ae_k = \lambda_k e_k.
	$$
	Сделаем некоторые преобразования:
	\begin{gather}
		\nonumber Ae_k - \lambda^*_i e_k = \lambda_k e_k - \lambda^*_i e_k, \implies \left(A - \lambda^*_i E \right) e_k = (\lambda_k - \lambda^*_i) e_k, \\
		\label{misha}
		\left(A - \lambda^*_i E \right)^{-1} e_k = \frac 1 {\lambda_k - \lambda^*_i} e_k.
	\end{gather}
	Отсюда следует, что $e_k$ является собственным вектором для матрицы $\left(A - \lambda_i E\right)^{-1}$, причем выполняется преобразование \eqref{misha}.
	
	В методе обратной итерации мы выбираем некоторое начальное приближение собственного вектора $x^{(0)}$для некоторого собственного значения $\lambda_i$, то есть предполагаем, что этот вектор можно разложить в линейную комбинацию собственных векторов $\{ e_k \}$ матрицы $A$ (о которых известно только их существование, вид неизвестен):
	\begin{equation}
		\label{summa}
		x^{(0)} = \sum_{k=1}^{n} c_k^{(0)} e_k.
	\end{equation}
	Метод обратной итерации можно охарактеризовать в виде
	$$
	\left( A - \lambda^*_i E \right ) y^{(k+1)} = x^{(k)},
	$$
	где $\lambda^*_i$ --- известное приближение некоторого собственного значения. Подставив \eqref{summa}, получим:
	\begin{equation}
		\label{svosvo}
		y^{(k+1)} = \left( A - \lambda^*_i E \right )^{-1} x^{(k)} = \left( A - \lambda^*_i E \right )^{-1} \sum_{m=1}^{n} c_m^{(k)} e_m.
	\end{equation}
	С учетом \eqref{misha}, выражение \eqref{svosvo} примет вид:
	$$
	y^{(k+1)} = \sum_{m=1}^{n} \frac{ c_m^{(k)} }{\lambda_m - \lambda^*_i} e_m, \qquad i \in \{1, \dots, n \}.
	$$
	
	Рассмотрим $k = 0$; $y^{(1)}$ имеет вид:
	$$
	y^{(1)} = \sum_{m=1}^{n} \frac{ c_m^{(0)} }{\lambda_m - \lambda^*_i} e_m = \sum_{m=1}^{n} c_m^{(1)}  e_m,
	$$
	откуда получаем $c^{(k)}_m = \dfrac{c^{(0)}_m}{(\lambda_m - \lambda^*_i)^k}$. Таким образом, при стремлении $k \to \infty$ и нормировке на каждом шаге алгоритма, в разложении вектора $x^{(k)} = \sum_{m=1}^{n} c_m^{(k)}  e_m$ коэффициент $c_i^{(k)} = \dfrac{ c_i^{(0)} }{(\lambda_i - \lambda^*_i)^k}$ при собственном векторе $e_i$ будет стремиться к единице, а остальные коэффициенты $c_m^{(k)}$ --- к нулю. То есть, вектор $x^{(k)}$ будет сходиться к собственному вектору $e_i$. В случае, если убрать нормировку из алгоритма, коэффициент $c_i^{(k)}$ будет стремиться в бесконечность, произойдет переполнение, и алгоритм сломается.
	
	
	\item \textbf{Приведите примеры использования собственных чисел и собственных векторов в численных методах}
	\vspace*{0.2cm}
	
	\textit{\textbf{Ответ:}}
	\begin{enumerate}
		\item Критерий сходимости стационарных итерационных методов решения СЛАУ (спектральный радиус меньше единицы); 
		\item Оптимальные итерационные параметры; для методов решения СЛАУ нередко требуют знания собственных значений матрицы (оптимальный параметр для метода простой итерации для симметричной, положительно определенной матрицы, для метода Ричардсона с Чебышевскими параметрами и т.д.);
		\item Оценка обусловленности матрицы;
		\item Решение системы линейных ОДУ с постоянными коэффициентами; 
		\item Привидение к канонической виду квадратичной формы. 
	\end{enumerate}
	
	\end{enumerate}
	

\item \textbf{Как понять в какой момент необходимо переходить к алгоритму со сдвигами? (Как в ходе вычислений определить, что отношение пары собственных значений близко к единице?) Как выбрать величину сдвига?}
\vspace*{0.2cm}

\textit{\textbf{Ответ:}}

\begin{enumerate}
	\item Можно, например, воспользоваться теоремой Гершгорина:  
	\[
	\abs{\lambda - a_{ii}} \le \sum\limits_{i \neq j}\abs{a_{ij}},
	\] 
	и переходить к сдвигам, если оценка меньше некоторого заданного числа.
	\item в ходе вычислений можно определить, что отношение пары собственных чисел мало, если круги Гершгорина при малом $\sum\limits_{i \neq j}\abs{a_{ij}}$ перекрывают друг друга.
	\item Во-первых, из постановки задачи или из физического смысла матрицы. Во-вторых, в процессе нахождения собственных значений возникают хорошие приближении.
\end{enumerate}

\item \textbf{Почему у одной и той же матрицы норма собственного вектора может быть как очень большой, так и очень маленькой?}

\vspace*{0.2cm}

\textit{\textbf{Ответ:}}

Норма собственного вектора может сильно варьироваться для одной и той же матрицы из-за произвольного выбора масштабирования собственного вектора: собственные векторы определяются с точностью до произвольного ненулевого множителя. Если $v$ - собственный вектор, то любой вектор $Cv$, где $C \neq 0$, также является собственным вектором, соответствующим тому же собственному значению. Это значит, что собственные векторы можно растягивать и сжимать, и их норма может быть произвольно большой или маленькой.

\item \textbf{Для десятого вопроса привести конкретные примеры с формулами.}
\vspace*{0.2cm}

\textit{\textbf{Ответ:}}

\begin{enumerate}
	\item Обусловленность матрицы: $\text{cond} A \ge \sfrac{\abs{\lambda_{\max} }}{\abs{\lambda_{\min} }}$
	\item Критерий сходимости итерационных методов решения СЛАУ: $\rho(C) < 1$.
	\item Решение системы дифференциальных уравнений с постоянными коэффициентами:
	\begin{gather}
		\dot X = A X \notag\\
		X = \sum\limits_{k=1}^{n} c_k e^{\lambda_k} \cdot X_k, \notag
	\end{gather}
	где $X_k$ --- собственный вектор $A$ соответствующий собственному значению~$\label_{k}$.
	\item Оптимальный итерационный параметр метода простой итерации: $\tau = \sfrac{2}{\lambda_{\max} + \lambda_{\min}}$.
\end{enumerate}



	
\end{document}